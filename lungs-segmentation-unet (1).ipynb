{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":19790,"sourceType":"datasetVersion","datasetId":14815},{"sourceId":20797,"sourceType":"datasetVersion","datasetId":15700},{"sourceId":4962811,"sourceType":"datasetVersion","datasetId":2878166},{"sourceId":9825091,"sourceType":"datasetVersion","datasetId":6025001},{"sourceId":339462,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":283870,"modelId":304712}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TensorFlow and Keras Imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, metrics\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Image Processing Libraries\nimport cv2\nfrom cv2 import imread,resize\nfrom scipy.ndimage import label, find_objects\n\n# Data Handling Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport random\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# File and Operating System Libraries\nimport os\n\n# Warnings Management\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU Configuration\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nprint(tf.config.list_physical_devices('GPU'))\n\n#DataGeneratrion\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# PATHS\nIMG_PATH = '/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/image'\nMSK_PATH = '/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/mask'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.test.is_gpu_available())\nprint(tf.test.is_built_with_cuda())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_img = cv2.imread('/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/image/1000.png',0)\nplt.imshow(sample_img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_img.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_msk = cv2.imread('/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/mask/1000.png',0)\nplt.imshow(sample_msk)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_msk.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Generation and processing","metadata":{}},{"cell_type":"code","source":"img_path = '/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/image'\nmsk_path = '/kaggle/input/chest-x-ray-lungs-segmentation/Chest-X-Ray/Chest-X-Ray/mask'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_files, msk_files = sorted(os.listdir(img_path)),sorted(os.listdir(msk_path))\n\ntrain_img_files,val_img_files,train_msk_files,val_msk_files = train_test_split(img_files,msk_files,test_size=0.2,random_state=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_datagen = ImageDataGenerator(\n    # rotation_range=90,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    # horizontal_flip=True,\n    # vertical_flip=True,\n    # shear_range=0.3,\n    # zoom_range=0.5,\n    fill_mode='reflect',\n    # rescale = 1./255.\n)\n\nmsk_datagen = ImageDataGenerator(\n    # rotation_range=90,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    # horizontal_flip=True,\n    # vertical_flip=True,\n    # shear_range=0.3,\n    # zoom_range=0.5,\n    fill_mode='reflect',\n    # rescale = 1./255.\n)\n\nbatch_size = 16\ntarget_size = (256,256)\n\ndef image_mask_generator(image_files,mask_files,batch_size):\n    while True:\n\n        combined = list(zip(image_files, mask_files))\n        random.shuffle(combined)\n        image_files, mask_files = zip(*combined)\n        \n        for i in range(0,len(image_files),batch_size):\n            batch_img_files = image_files[i:i+batch_size]\n            batch_msk_files = mask_files[i:i+batch_size]\n\n            msk_batch = []\n            img_batch = []\n\n            for img_file, msk_file in zip(batch_img_files,batch_msk_files):\n                img = cv2.imread(os.path.join(img_path,img_file), cv2.IMREAD_COLOR)\n                img = cv2.resize(img,target_size)/255.\n                # img = np.expand_dims(img, axis = -1)\n\n                msk = cv2.imread(os.path.join(msk_path,msk_file), cv2.IMREAD_GRAYSCALE)\n                msk = cv2.resize(msk,target_size)/255.\n                msk = (msk>0).astype(np.uint8)\n                msk = np.expand_dims(msk, axis = -1)\n\n                img_batch.append(img)\n                msk_batch.append(msk)\n\n            img_batch = np.array(img_batch)\n            msk_batch = np.array(msk_batch)\n\n            # seed = np.random.randint(1,1000)\n            # img_batch = next(img_datagen.flow(img_batch, batch_size = batch_size, seed=seed, shuffle = False))\n            # msk_batch = next(msk_datagen.flow(msk_batch, batch_size = batch_size, seed=seed, shuffle = False))\n\n            yield img_batch, msk_batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\n\n# Function to visualize the original and augmented image-mask pairs\ndef visualize_generator(generator, image_files, mask_files, num_samples=3):\n    \"\"\"\n    Plots original images and masks alongside augmented versions.\n    \"\"\"\n    # Load a batch of images & masks from generator\n    img_batch, msk_batch = next(generator)\n\n    # Show the first `num_samples` images and masks\n    for i in range(num_samples):\n        img = cv2.imread(os.path.join(img_path, image_files[i]), cv2.IMREAD_COLOR)\n        img = cv2.resize(img, target_size) / 255.0  # Normalize\n\n        msk = cv2.imread(os.path.join(msk_path, mask_files[i]), cv2.IMREAD_GRAYSCALE)\n        msk = cv2.resize(msk, target_size) / 255.0  # Normalize\n        msk = (msk > 0).astype(np.uint8)  # Binarize mask\n\n        aug_img = img_batch[i]\n        aug_msk = msk_batch[i]\n\n        plt.figure(figsize=(8, 4))\n\n        # Original Image\n        plt.subplot(2, 2, 1)\n        plt.imshow(img)\n        plt.title(\"Original Image\")\n        plt.axis(\"off\")\n\n        # Original Mask\n        plt.subplot(2, 2, 2)\n        plt.imshow(msk, cmap=\"gray\")\n        plt.title(\"Original Mask\")\n        plt.axis(\"off\")\n\n        # Augmented Image\n        plt.subplot(2, 2, 3)\n        plt.imshow(aug_img)\n        plt.title(\"Augmented Image\")\n        plt.axis(\"off\")\n\n        # Augmented Mask\n        plt.subplot(2, 2, 4)\n        plt.imshow(aug_msk.squeeze(), cmap=\"gray\")\n        plt.title(\"Augmented Mask\")\n        plt.axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n\n# Create the generator\ntest_generator = image_mask_generator(train_img_files, train_msk_files, batch_size=8)\n\n# Visualize results\nvisualize_generator(test_generator, train_img_files, train_msk_files)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = image_mask_generator(train_img_files, train_msk_files, batch_size)\nvalid_generator = image_mask_generator(val_img_files, val_msk_files, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unet(input_shape, num_classes):\n    # Input layer for the model\n    inputs = layers.Input(shape=input_shape)\n    \n    # Contracting Path (Encoder)\n    # First block with 64 filters\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = layers.BatchNormalization()(conv1)  # Add Batch Normalization\n    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n    conv1 = layers.BatchNormalization()(conv1)  # Add Batch Normalization\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)  # Downsample\n\n    # Second block with 128 filters\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = layers.BatchNormalization()(conv2)  # Add Batch Normalization\n    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n    conv2 = layers.BatchNormalization()(conv2)  # Add Batch Normalization\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv2)  # Downsample\n\n    # Third block with 256 filters\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = layers.BatchNormalization()(conv3)  # Add Batch Normalization\n    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n    conv3 = layers.BatchNormalization()(conv3)  # Add Batch Normalization\n    pool3 = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv3)  # Downsample\n\n    # Bottleneck layer with 512 filters\n    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = layers.BatchNormalization()(conv4)  # Add Batch Normalization\n    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n    conv4 = layers.BatchNormalization()(conv4)  # Add Batch Normalization\n\n    # Expansive Path (Decoder)\n    # First upsampling block\n    up6 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4)  # Upsample\n    merge6 = layers.concatenate([up6, conv3])  # Concatenate with the corresponding encoder block\n    conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge6)\n    conv6 = layers.BatchNormalization()(conv6)  # Add Batch Normalization\n    conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n    conv6 = layers.BatchNormalization()(conv6)  # Add Batch Normalization\n\n    # Second upsampling block\n    up7 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6)  # Upsample\n    merge7 = layers.concatenate([up7, conv2])  # Concatenate with the corresponding encoder block\n    conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge7)\n    conv7 = layers.BatchNormalization()(conv7)  # Add Batch Normalization\n    conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n    conv7 = layers.BatchNormalization()(conv7)  # Add Batch Normalization\n\n    # Third upsampling block\n    up8 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7)  # Upsample\n    merge8 = layers.concatenate([up8, conv1])  # Concatenate with the corresponding encoder block\n    conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge8)\n    conv8 = layers.BatchNormalization()(conv8)  # Add Batch Normalization\n    conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n    conv8 = layers.BatchNormalization()(conv8)  # Add Batch Normalization\n\n    # Output layer for segmentation\n    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(conv8)\n\n    # Create the model\n    model = models.Model(inputs=[inputs], outputs=[outputs])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Jaccard Index Metric\ndef jaccard_index(y_true, y_pred, smooth=100):\n    \"\"\"Calculates the Jaccard index (IoU), useful for evaluating the model's performance.\"\"\"\n    y_true_f = tf.reshape(tf.cast(y_true, tf.float32), [-1])  # Flatten and cast ground truth\n    y_pred_f = tf.reshape(tf.cast(y_pred, tf.float32), [-1])  # Flatten and cast predictions\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)  # Compute intersection\n    total = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection  # Total pixels\n    return (intersection + smooth) / (total + smooth)\n\n\ndef dice_coefficient(y_true, y_pred, smooth=1):\n    # Flatten and cast true and predicted masks to float32\n    y_true_f = tf.reshape(tf.cast(y_true, tf.float32), [-1])  # Flatten and cast y_true to float32\n    y_pred_f = tf.reshape(tf.cast(y_pred, tf.float32), [-1])  # Flatten and cast y_pred to float32\n    \n    # Calculate the intersection between the true and predicted masks\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    \n    # Calculate the Dice coefficient using the formula\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.0\n    y_true_f = tf.reshape(tf.cast(y_true, tf.float32), [-1])\n    y_pred_f = tf.reshape(tf.cast(y_pred, tf.float32), [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n\n# Using the metrics in model compilation\n\ninput_shape = (256,256,3)\n\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():  # Use GPUs for training\n    model = unet(input_shape, 1)\n    model.compile(optimizer='adam', \n                  loss='binary_crossentropy', \n                  metrics=['accuracy',dice_coefficient,jaccard_index])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## ModelCheckpoint: Save the model with the best validation loss during training\ncheckpoint = ModelCheckpoint(\n    'best_model.keras',            # Path to save the model\n    monitor='val_dice_coefficient',         # Metric to monitor \n    verbose=1,                  # Print messages when saving the model\n    save_best_only=True,        # Save only the best model (with highest metric)\n    mode='max',                 # 'max' means the model with the highest metric score will be saved\n    save_weights_only=False,     # Save the entire model (not just weights)\n)\n\n\nhistory = model.fit(\n    train_generator,                  # The training data generator or dataset\n    validation_data=valid_generator,\n    steps_per_epoch=len(train_img_files) // batch_size,\n    validation_steps=len(val_img_files) // batch_size,\n    epochs=150,              # Number of epochs\n    callbacks=[checkpoint],# Callbacks for checkpoint and early stopping\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16,16))\nplt.subplot(221)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n\nplt.subplot(222)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Metric Value')\nplt.xlabel('Epoch')\n\nplt.subplot(223)\nplt.plot(history.history['dice_coefficient'], label='Train Dice Coefficient')\nplt.plot(history.history['val_dice_coefficient'], label='Validation Dice Coefficient')\nplt.title('Model Dice Coefficient')\nplt.ylabel('Metric Value')\nplt.xlabel('Epoch')\n\nplt.subplot(224)\nplt.plot(history.history['jaccard_index'], label='Train Jaccard Index')\nplt.plot(history.history['val_jaccard_index'], label='Validation Jaccard Index')\nplt.title('Model Jaccard Index')\nplt.ylabel('Metric Value')\nplt.xlabel('Epoch')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nimport timm  # For Vision Transformer\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport random\n\n# Custom Metrics\ndef jaccard_index(y_true, y_pred, smooth=100):\n    y_true_f = tf.reshape(tf.cast(y_true, tf.float32), [-1])\n    y_pred_f = tf.reshape(tf.cast(y_pred, tf.float32), [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    total = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n    return (intersection + smooth) / (total + smooth)\n\ndef dice_coefficient(y_true, y_pred, smooth=1):\n    y_true_f = tf.reshape(tf.cast(y_true, tf.float32), [-1])\n    y_pred_f = tf.reshape(tf.cast(y_pred, tf.float32), [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n# Load the segmentation model (change the path as needed)\nsegment_model = load_model('/kaggle/input/c-fvgfb-ghhjfjjhfjh/keras/default/1/model.keras',\n                            custom_objects={'dice_coefficient': dice_coefficient, 'jaccard_index': jaccard_index})\n\n# Load data from CSV\ndata_df = pd.read_csv('/kaggle/input/tbx11k-simplified')\nactive_tb = data_df[data_df['tb_type']=='active_tb']['fname'].tolist()\nactive_tb = list(pd.unique(active_tb))\nnormal = data_df[data_df['image_type']=='healthy']['fname'].tolist()\nimg_path = '/kaggle/input/tbx11k-simplified/tbx11k-simplified/images'\n\nimg_list = []\nlabels = []\nselected_normal = random.sample(normal, len(active_tb))\nprint(f\"shape of selected_normal: {len(selected_normal)}\")\nprint(f\"shape of active_tb: {len(active_tb)}\")\n\n# Process images for each class\nfor filename in selected_normal:\n    img = cv2.imread(os.path.join(img_path, filename))\n    img = cv2.resize(img, (256, 256)) / 255.0  # Normalize to [0,1]\n    img_list.append(img)\n    labels.append(0)\nfor filename in active_tb:\n    img = cv2.imread(os.path.join(img_path, filename))\n    img = cv2.resize(img, (256, 256)) / 255.0\n    img_list.append(img)\n    labels.append(1)\n\nimg_list = np.array(img_list)\nlabels = np.array(labels)\n\n# Shuffle and split\nshuffle_indices = np.random.permutation(len(img_list))\nimg_list = img_list[shuffle_indices]\nlabels = labels[shuffle_indices]\nX_train, X_val, y_train, y_val = train_test_split(img_list, labels, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.2, random_state=42)\n\n##############################\n# Segmentation Preprocessing\n##############################\n\ndef segment_and_preprocess(img):\n    # Convert normalized image to uint8\n    img_uint8 = (img * 255).astype(np.uint8)\n    # Convert BGR to grayscale\n    img_gray = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2GRAY)\n    # Resize grayscale image to (48, 40)\n    img_resized = cv2.resize(img_gray, (48, 40))\n    # Expand dims to get shape (1, 40, 48, 1)\n    img_resized = np.expand_dims(np.expand_dims(img_resized, axis=-1), axis=0)\n    # Flatten to match segmentation model input\n    img_resized_flattened = img_resized.flatten().reshape(1, 1920)\n    \n    # Create dummy input for the model's second input (shape: (1, 34))\n    dummy_input = np.zeros((1, 34))\n    \n    try:\n        # Predict segmentation mask (raw_mask is (8485,))\n        raw_mask = segment_model.predict([img_resized_flattened, dummy_input], verbose=0)[0]\n        print(\"Raw mask shape:\", raw_mask.shape)  # Should print (8485,)\n        \n        # Resize this flat mask to 1920 elements\n        new_length = 40 * 48  # 1920 elements desired\n        x_old = np.linspace(0, 1, raw_mask.shape[0], endpoint=False)\n        x_new = np.linspace(0, 1, new_length, endpoint=False)\n        mask_resized_flat = np.interp(x_new, x_old, raw_mask)\n        \n        # Threshold the interpolated mask\n        mask_thresholded = (mask_resized_flat > 0.5).astype(np.float32)\n        # Reshape to (40, 48)\n        mask_reshaped = mask_thresholded.reshape(40, 48)\n        print(\"Reshaped mask shape:\", mask_reshaped.shape)  # Should print (40, 48)\n        return mask_reshaped\n        \n    except ValueError as e:\n        print(f\"Error during prediction: {e}\")\n        return None\n\n# Apply segmentation preprocessing\nX_train_processed = np.array([segment_and_preprocess(img) for img in tqdm(X_train)])\nX_val_processed = np.array([segment_and_preprocess(img) for img in tqdm(X_val)])\nX_test_processed = np.array([segment_and_preprocess(img) for img in tqdm(X_test)])\n\nprint(\"Shape of processed X_train:\", X_train_processed.shape)\nprint(\"Shape of processed X_val:\", X_val_processed.shape)\nprint(\"Shape of processed X_test:\", X_test_processed.shape)\n\n#############################################\n# Data Augmentation and Classification Setup\n#############################################\n\naugmentor = ImageDataGenerator(\n    horizontal_flip=True,\n    zoom_range=0.1,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\n\ntrain_gen = augmentor.flow(X_train_processed, y_train, batch_size=32)\nval_gen = tf.data.Dataset.from_tensor_slices((X_val_processed, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Define ViT Model from timm library\nvit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n\n# Add classification head on top of ViT\nx = vit_model.get_classifier()(vit_model.output)\nx = tf.keras.layers.GlobalAveragePooling1D()(x)  # Optional if you want to pool the features before classification\nx = tf.keras.layers.Dropout(0.3)(x)\nout = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Binary classification output\nmodel = tf.keras.Model(inputs=vit_model.input, outputs=out)\n\n# Compile model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n)\n\n# Early stopping and learning rate scheduling\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, mode='max', restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', patience=2, factor=0.2, mode='max', min_lr=1e-6)\n\n# Train only classification head first\nmodel.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    callbacks=[early_stop, reduce_lr]\n)\n\n# Unfreeze and fine-tune the full model\nvit_model.trainable = True\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n)\n\nmodel.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=20,\n    callbacks=[early_stop, reduce_lr]\n)\n\nmodel.save('vit_tb_classifier_segmented.keras')\n\n# Evaluate and visualize on test set\ny_pred_probs = model.predict(X_test_processed)\ny_pred = (y_pred_probs > 0.5).astype(int)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\nroc_auc = roc_auc_score(y_test, y_pred_probs)\nfpr, tpr, _ = roc_curve(y_test, y_pred_probs)\nplt.figure(figsize=(6, 4))\nplt.plot(fpr, tpr, color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\nplt.title(f\"ROC Curve (AUC = {roc_auc:.3f})\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\nimgs, labels = next(iter(train_loader))\nimgs = imgs.to(device)\nlabels = labels.float().unsqueeze(1).to(device)\n\nfor step in range(100):\n    optimizer.zero_grad()\n    outputs = model(imgs).squeeze()\n    loss = criterion(outputs, labels.view(-1))\n    loss.backward()\n    optimizer.step()\n    print(f\"Step {step+1} - Loss: {loss.item():.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:51:28.881048Z","iopub.execute_input":"2025-04-19T06:51:28.882103Z","iopub.status.idle":"2025-04-19T06:51:28.961799Z","shell.execute_reply.started":"2025-04-19T06:51:28.882061Z","shell.execute_reply":"2025-04-19T06:51:28.960796Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/928169150.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"segment_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}